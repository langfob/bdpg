---
title: "BDPG paper skeleton"
author: "Bill Langford"
date: "`r Sys.Date()`"
output:
  pdf_document:
    fig_caption: yes
    number_sections: yes
    toc: yes
    toc_depth: 4
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '4'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#  Abstract

#  Introduction

##  Reserve selection commonly used but without any measured estimates of solution quality under uncertainty
- Optimizer performance is only guaranteed under data without uncertainty.
- But there's a lot of uncertainty in inputs
- Some attempts to address this, but no generally test and predict performance under uncertainty
- While reserve selection is intended as a decision aid rather than a decision-making device, the tacit assumption is that the solutions offered have at least reasonable quality, but this has never been evaluated.


##  Can't predict how reliable specific RS is on specific problem   
- Toy problems
    - Little complexity
    - No acknowledgement or consideration of variability in problem difficulty
- Currently no work looking at even *mean* performance and variability under uncertainty across multiple problems
- Absolutely no work on predicting error for arbitrary *specific* problems

## Prediction requires several things that are missing now
- Lots of training data with known solutions and error characteristics
    - Training data has to represent the range of problems in the universe to be generalized over
    - Need to generate many problems of varying difficulty with known solutions
        - Difficulties generating problems with known solutions
            - Currently can't predictably vary problem difficulty
            - Can run ILP like Gurobi but too slow for large numbers of problems
- Measurable attributes of problems
    - Currently only know # of spp and # of PUs
    - Need other measures that *can* be known
    - Might assume that input error amount/type are known, but ...

##  Here, we describe a method for learning to predict the amount of output error
- Generating problems of varying difficulty with known solutions
- New input features for predicting problem difficulty



All data and R source code [cite R team] for this paper are open source and available under the [xyz] license. The R code is at [github].  The data is at [someplace].

To our knowledge, this work is novel in 1) raising the question of the relative difficulty of different reserve selection problems, 2) mapping methods from the theory of computation for generating reserve selection problems with known difficulty and known solutions, 3) extending those methods to allow more ecologically realistic species distributions, 4) attempting to predict output error of specific reserve selectors on specific reserve selection problems, 5) identifying bipartite network complexity measures useful in predicting reserve selection output error, and 6) attempting to predict the amount of error in reserve selector output for specific given problems.  While this is only a first step in being able to predict the accuracy of particular reserve selectors on particular reserve selection problems in general, it is a significant step in showing its importance and possibility



#  Methods

## Background

A simplest form of the reserve selection problem is the following: given a set of species occurrences on a set of patches, find the smallest subset of the patches that contains at least one occurrence of each species.  In more general forms of the problem, 1) "species" could be any conservation feature, 2) "patches" are often referred to as planning units, 3) occurrence targets for features could have values other than one, 4) planning units could have unequal costs, and 5) there could be other constraints on the solution such as minimizing total boundary length.  Here, we will only consider the simple form because it facilitates generating problems with known solutions and it's already complex enough to cause reserve selectors substantial difficulties.  Future work will consider more complex problems.

The simple form of reserve selection is related to a problem known in theory of computation as Minimum Set Cover, which belongs to a heavily-studied class of difficult problems known as NP-Complete or NP-Hard depending on the exact phrasing of each problem.  The exact definitions of these complexity classes and their details are not necessary for understanding the methods presented here and can be found in any text on theory of computation or algorithm design.  However, the study of those classes has led to two things important to the work described here.  First, certain problems have a form of equivalance, in that given elements of one problem, we can find corresponding elements in another kind of problem and the solution there can be mapped back as a solution of the original problem.  Second, the large variation in difficulty among specific instances of each problem type has led to work on predicting relative problem difficulty and generating problems of differing difficulties with known solutions.  

One technique for generating difficult problems is "solution hiding".  There, a problem is generated in a way that has an obvious solution, then, obfuscating elements are added to the problem in a way that they don't change the solution but they make searching for it difficult.  As a simple analogy, suppose there was a 10,000 hectare forest of nothing but pine trees, except for three rose bushes and the "problem" was to find the three bushes.  If the problem generator started with an empty landscape, planted the three bushes at locations they knew and then planted the pine forest afterwards, the generator would easily know the solution but make it difficult for the searcher to find.  

Xu et al.[xxx] describes a solution-hiding algorithm for generating instances of a well-known problem called Satisfiability.  Importantly, they also provide theoretical predictions of the relative difficulty of each problem based on the constructor's parameters.  Elsewhere [xxx], Xu briefly describes a variation of the generator that is extremely simple and generates instances with known solutions for a difficult graph problem called Maximum Independent Set.  This is the first step in mapping from problem to problem to find a way to generate reserve selection problems of known difficulty with known solutions.  We give more details below, but the basic idea is that the solution of the Maximum Independent Set problem is known to be the complement of the solution to another graph problem called Minimum Vertex Cover.  Minimum Vertex Cover is in turn, a special case of the Minimum Set Cover problem, which brings us back to reserve selection.  Consequently, we have a very simple algorithm for generating reserve selection problems of varying difficulty and known solutions.

There are two major drawbacks to problems generated with the XXXX method.  First, it only generates problems where *every* species in *every* problem occurs on exactly 2 patches.  Since this is ecologically unrealistic, we modified the XXXX method to allow the wrapping of nearly any species distribution around XXXX problems while preserving the known optimal solution.  The second ecologically unrealistic attribute of these problems is that there is no error or uncertainty in the occurrence and cost data.  Consequently, we built a series of simple models of false positive and false negative input errors and cost errors and applied them to the data.   

All of this gives us the skeleton for generating non-trivial reserve selection problems with known solutions and approximately known difficulty so that we can properly evaluate the performance of different reserve selectors.  However, what we would really like to know is, given a particular reserve selection problem and reserve selector, can we predict how much error is likely in the specific solution to this specific problem?  By error, we mean two things: 1) How much does it fall short of its representation targets? and 2) How much more does it cost than the correct optimal solution?  

To answer these questions for specific problems, we need to identify descriptors for each problem that are knowable or measurable without knowing the correct solution.  To our knowledge, the only descriptors people might currently use to guess the likely performance of a reserve selector would be related to the number of species and/or the number of patches.  We show here that, in our experiments, these are virtually useless as predictors of problem difficulty.  Instead, we reframe the problem as a bipartite network of species and patches, analogous to networks used in studying plant-pollinator interactions.  We then compute network metrics related to the relative complexity of these networks and show their utility in our experiments for predicting output errors in representation and cost.  

## Problem set generation

In this section, we give an overview of the structure and number of experiments.  Details of each component are given in later sections.  For this paper, we generated a set of problems that we could use to do initial explorations of the data as well as test the code under a fairly large number of conditions.  Later papers will formally define universes of problems and sample from those universes to learn and attempt to generalize over them using the methods developed in this paper.  

We developed problems with known correct answers in blocks of 20 problems, where each of the 20 is derived from a single base problem by adding specific complexities and uncertainties (described below).  We then repeated stochastic variants of the block generation procedure 100 times for each of 5 different input error levels and 2 different problem difficulty levels. This led to the generation of 20 * 100 * 5 * 2 = 20,000 different reserve selection problems.  For each problem, we then applied 9 different reserve selection methods.  

Each block of 20 problems began with:

- 1 Base problem generated using the XXXX method and no input error
- 1 Wrapped problem derived from the Base problem and no input error

These are referred to as the correct or COR problems.

For each of those problems, we created 9 new variations using different combinations of input errors at a single specified rate:

- Cost error only
- Each of the 4 below with and without cost error for a total of 8 variants
    - False negatives only
    - False positives only
    - Both false negative and false positive at the given error rate
    - Both false negative and false positive with error counts matched
    
These are referred to as the apparent or APP problems since they are the data that would be apparent to the user in a real situation where the correct data is unknown.

## Individual problem generation

Generating individual problems relies on the idea of the *maximum independent set*.  In a network, an independent set is any set of nodes where none of those nodes are connected to any other nodes *in that set*.  The *maximum* independent set for a network is defined to be the largest such set possible for that network.  

###  XXXX base problem generation

We generated each base problem by generating a Maximum Independent Set problem with a known optimal solution using the methods given in [XuPaper][XuWebsite].  We then mapped the network into a corresponding reserve selection problem and used the complement of the independent set solution as the optimal solution to the reserve selection problem.  The mapping is as follows:

- Each node in the network corresponds to a unique patch.
- Each link corresponds to a unique species.  
    - That species is designated as occurring in each of the two patches connected by the link.
- Every node/patch NOT in the independent set is included in the optimal solution to the reserve selection problem.

####  Base network generation procedure

The basic idea was to build a set of identical groups of nodes whose structure forced exactly one element of each group to be the independent set for that group.  That way, when we combined the groups, the maximum independent set was just the union of the individual independent nodes.  Finally, we added complexity to this network by adding extra links between the groups, excluding the independent nodes so that they are still the maximum independent set for the more complicated network.

- Define a network that is a collection of **g** identically structured groups of nodes and links, where:
    - Each group consists of **k** nodes where each node has a link to every other node in the group.
- Arbitrarily choose one node in each group and designate each as the independent node for that group.
- For a given number of rounds **r**: 
    - Randomly choose a pair of groups
    - For a given number of linkings **s**:
        - Randomly choose a non-independent node in each of the chosen groups and link them.  (Multiple links between the same node pairs are allowed.)

Note that:

- The union of the group independent nodes is the maximum independent set.
- The optimal reserve selection is the set of g * (k - 1) nodes not in the maximum independent set.

####  Parameter choice

The size and relative difficulty of the problem generated is determined by the choice of parameters in the procedure above.  We derived these parameters from 4 parameters in the XXXX method described in [XuPaper].  The parameters there are driven by theoretical considerations to control problem difficulty and are as follows:

- **alpha** ...
- **N** ...
- **r** ...
- **p** ...

For the experiments in this paper, we chose two distinct sets of values that corresponded to what Xu et al had found to generate easy problems and hard problems:

- EASY
    - **alpha** = ...
    - **N** = ...
    - **r** = ...
    - **p** = ...
- HARD
    - **alpha** = ...
    - **N** = ...
    - **r** = ...
    - **p** = ...

Those values were then converted into the values necessary for the procedure above:

- **g** = ...
- **k** = ...
- **r** = ...
- **s** = ...

giving:

- EASY
    - **g** = ...
    - **k** = ...
    - **r** = ...
    - **s** = ...
- HARD
    - **g** = ...
    - **k** = ...
    - **r** = ...
    - **s** = ...

We made one other modification to these values.  Since the solution size is always g \* (k - 1)  and the part of the landscape not in the solution is only g, some algebra shows that the fraction of the landscape included in the solution = 1 - 1/k.  So, as the size of the groups increases, so does the fraction of the landscape included in the solution.  These large fractions are not ecologically realistic as an amount to reserve, so we fixed k = 2 to get the smallest possible size (1/2) for the solution fraction of the landscape for these experiments.  In the future, we would not repeat this choice when the emphasis is on the wrapped problems since we can directly choose a realistic landscape fraction there and this choice of k moves us away from the predictive choices specified in [XuPaper].   

###  Wrapped problem generation

The XXXX method gave us an ecologically unrealistic, completely flat species distribution with exactly 2 occurrences of every species as well as unrealistically large fractions of the landscape to reserve.  To generate more realistic problems, we developed a method for wrapping a base XXXX problem in a larger species distribution and landscape.  

The basic idea for wrapping is to note that any given species abundance distribution will have one section of the curve where it specifies a certain number of species that occur on exactly two patches.  To generate a wrapped problem then, we need to: 

- Search for a curve that has a section that specifies roughly the same number of species that occur on exactly 2 patches as we have in our base XXXX problem and then,
- Add more species that correspond to the rest of the curve and 
- While making sure that:
    - each of the new species has at least one occurrence in the original optimal reserve selection and 
    - none of the original problem's species occurs outside the original problem's set of patches.
    
We will generally also add more patches to the landscape as a way to control what fraction of the total landscape will be taken up by the optimal reserve selection.  

####  Computing number of patches to add

Given a desired solution fraction and an original XXXX problem:

- (total landscape size) = (ind set size) + (soln size) + (num extra patches), and 
- (soln frac) = (soln size) / (total landscape size) => 

- (soln frac)((ind set size) + (soln size) + (num extra patches)) = (soln size) =>
- (soln frac)(num extra patches) = (soln size) - ((soln frac)(ind set size) + (soln frac)(soln size)) =>
- (num extra patches) = ((soln size) - ((soln frac)(ind set size) + (soln frac)(soln size))) / (soln frac) =>
- (num extra patches) = (soln size)/(soln frac) - (ind set size) - (soln size) =>
- **(num extra patches)** = (soln size)/(soln frac) - (XXXX prob size)

#### Species abundance distribution

To our knowledge, there are unfortunately no studies of what species distributions do occur in real reserve selection problems.  While species distributions in general are studied, the choice of which *subset* of the full distribution to include in a reserve selection problem is completely at the discretion of whoever is doing the selection.  Consequently, for our experiments we had to arbitrarily choose what species distribution to use as something more realistic than two occurrences for every species.  A lognormal is one commonly discussed model of species abundance distribution, so we chose that as an example abundance distribution for our experiments.  

####  Spatial distribution of species

Again, to our knowledge, there are no studies of the spatial co-occurence distributions of species in real reserve selection problems.  Given no reason to choose any particular spatial distribution, we randomly assigned new species occurrences to patches, with two caveats:

- No occurrence could be added to any patch in the XXXX problem's independent set.  
    - This is probably slightly stricter than necessary, but is guaranteed not to violate the original solution.
- The first added occurrence for any new species had to go on a patch in the XXXX solution so that the original solution was guaranteed to also be a solution for the wrapped problem.
    - Subsequent occurrence additions for new species were assigned to the extra patches set, though again, this is stricter than necessary.  

####  No singleton species

If any species occurs on just one patch, then that patch must be included in the solution to insure that all species are represented in the solution.  For this reason, we didn't add any species that occurred on just one patch.

###  Input error generation

####  Input error types

####  Nominal vs. Realized errors

## Problem measures

###  Problem size

###  Graph/Network complexity measures

For each problem, we converted the problem to a bipartite network and measured various network complexity metrics on the network to use in attempting to predict reserve selector output error.

####  igraph package measures

Latapy redundancy measures and the web page used as the basis for the R code 

####  bipartite package measures

##  Problem difficulty measures

###  Output error measures

####  Fractional error

####  Error magnification

###  Run-time measures

##  Reserve selectors

###  Primary reserve selectors

###  Less important greedy selectors

##  Prediction 

###  Values to predict

###  Prediction methods

####  Linear models

####  Regression trees

####  Random forest

##  Running and managing many experiments

###  Number of experiments

###  Tzar

###  Nectar

#  Results

##  Input errors vs. output errors

###  Errors have huge variability regardless of input error amount and type

###  Optimization often amplifies input error

####  Error amplification

####  Error correction

##  Error predictability

###  Problem size is poor predictor

###  Graph measures appear to predict well

##  Discussion

##  Caveats

###  No generalization beyond this data set

Only intended as starting point to test the code and explore general behavior.

##  Lit review

###  Future work

####  Need broader random sample within Xu universe to generalize there

Running more experiments now.

####  Need more realistic distributions and problems (e.g., boundary lengths)

Will ask Marxan community for input files since there are lots of them applied to lots of real problems and all use the same common input file format and it's easy to anonymize.

#  Conclusion

##  Contributions
- Mapping of methods from theory of computation into reserve selection for problem generation accounting for
    - varying problem difficulty
    - known solutions
    - scalable generation time
- New method for wrapping Xu problems to make them more realistic for ecology
- Mapping bipartite network attributes into problem difficulty prediction in reserve selection
- Reasonably large first study with both easy and hard problems and tested predictions of error
- Reproducible data and R package code

##  Recommendations
- Test and predict the robustness of methods to uncertainty and problem difficulty
- Make optimization evaluation reflect true goals
    - If we're going to declare reserve selectors to be decision-*support* tools rather than decision-*making* tools, then evaluation functions used in optimization should reflect what it means for a solution or solution set to be optimal as decision support.  The current phrasing of optimality measures tacitly assume optimality is how well invidual solutions meet the cost and representation targets specified for them, independent of other solutions and of input uncertainty.  Instead, they should explicitly state and measure what it would mean for a single solution or a set of solutions to be optimal as decision advice.  For example, they could include things like:  
        - a measure of the *variety* in solutions produced (while still being decent individual solutions), or 
        - the measured *robustness* of solutions to relative problem difficulty and to uncertainty (of all kinds or specific kinds and amounts).

#  Software and data availability and reproducibility

##  R bdpg package on github

##  data on ???


#  References

#  Supplementary Material

