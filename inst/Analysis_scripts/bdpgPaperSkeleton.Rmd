---
title: "BDPG paper skeleton"
author: "Bill Langford"
date: "`r Sys.Date()`"
output:
  pdf_document:
    fig_caption: yes
    number_sections: yes
    toc: yes
    toc_depth: 4
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '4'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#  Abstract

#  Introduction

##  Reserve selection commonly used but without any measured estimates of solution quality under uncertainty
- Optimizer performance is only guaranteed under data without uncertainty.
- But there's a lot of uncertainty in inputs
- Some attempts to address this, but no generally test and predict performance under uncertainty
- While reserve selection is intended as a decision aid rather than a decision-making device, the tacit assumption is that the solutions offered have at least reasonable quality, but this has never been evaluated.


##  Can't predict how reliable specific RS is on specific problem   
- Toy problems
    - Little complexity
    - No acknowledgement or consideration of variability in problem difficulty
- Currently no work looking at even *mean* performance and variability under uncertainty across multiple problems
- Absolutely no work on predicting error for arbitrary *specific* problems

## Prediction requires several things that are missing now
- Lots of training data with known solutions and error characteristics
    - Training data has to represent the range of problems in the universe to be generalized over
    - Need to generate many problems of varying difficulty with known solutions
        - Difficulties generating problems with known solutions
            - Currently can't predictably vary problem difficulty
            - Can run ILP like Gurobi but too slow for large numbers of problems
- Measurable attributes of problems
    - Currently only know # of spp and # of PUs
    - Need other measures that *can* be known
    - Might assume that input error amount/type are known, but ...

##  Here, we describe a method for learning to predict the amount of output error
- Generating problems of varying difficulty with known solutions
- New input features for predicting problem difficulty



All data and R source code [cite R team] for this paper are open source and available under the [xyz] license. The R code is at [github].  The data is at [someplace].

To our knowledge, this work is novel in 1) raising the question of the relative difficulty of different reserve selection problems, 2) mapping methods from the theory of computation for generating reserve selection problems with known difficulty and known solutions, 3) extending those methods to allow more ecologically realistic species distributions, 4) attempting to predict output error of specific reserve selectors on specific reserve selection problems, and 5) identifying bipartite network complexity measures useful in predicting reserve selection output error.  While this is only a first step in being able to predict the accuracy of particular reserve selectors on particular reserve selection problems in general, it is a significant step in showing its importance and possibility



#  Methods

## Background

A simplest form of the reserve selection problem is the following: given a set of species occurrences on a set of patches, find the smallest subset of the patches that contains at least one occurrence of each species.  In more general forms of the problem, 1) "species" could be any conservation feature, 2) "patches" are often referred to as planning units, 3) occurrence targets for features could have values other than one, 4) planning units could have unequal costs, and 5) there could be other constraints on the solution such as minimizing total boundary length.  Here, we will only consider the simple form because it facilitates generating problems with known solutions and it's already complex enough to cause reserve selectors substantial difficulties.  Future work will consider more complex problems.

The simple form of reserve selection is related to a problem known in theory of computation as Minimum Set Cover, which belongs to a heavily-studied class of difficult problems known as NP-Complete or NP-Hard depending on the exact phrasing of each problem.  The exact definitions of these complexity classes and their details are not necessary for understanding the methods presented here and can be found in any text on theory of computation or algorithm design.  However, the study of those classes has led to two things important to the work described here.  First, certain problems have a form of equivalance, in that given elements of one problem, we can find corresponding elements in another kind of problem and the solution there can be mapped back as a solution of the original problem.  Second, the large variation in difficulty among specific instances of each problem type has led to work on predicting relative problem difficulty and generating problems of differing difficulties with known solutions.  

One technique for generating difficult problems is "solution hiding".  There, a problem is generated in a way that has an obvious solution, then, obfuscating elements are added to the problem in a way that they don't change the solution but they make searching for it difficult.  As a simple analogy, suppose there was a 10,000 hectare forest of nothing but pine trees, except for three rose bushes and the "problem" was to find the three bushes.  If the problem generator started with an empty landscape, planted the three bushes at locations they knew and then planted the pine forest afterwards, the generator would easily know the solution but make it difficult for the searcher to find.  

Xu et al.[xxx] describes a solution-hiding algorithm for generating instances of a well-known problem called Satisfiability.  Importantly, they also provide theoretical predictions of the relative difficulty of each problem based on the constructor's parameters.  Elsewhere [xxx], Xu briefly describes a variation of the generator that is extremely simple and generates instances with known solutions for a difficult graph problem called Maximum Independent Set.  This is the first step in mapping from problem to problem to find a way to generate reserve selection problems of known difficulty with known solutions.  We give more details below, but the basic idea is that the solution of the Maximum Independent Set problem is known to be the complement of the solution to another graph problem called Minimum Vertex Cover.  Minimum Vertex Cover is in turn, a special case of the Minimum Set Cover problem, which brings us back to reserve selection.  Consequently, we have a very simple algorithm for generating reserve selection problems of varying difficulty and known solutions.

There are two major drawbacks to problems generated with the XXXX method.  First, it only generates problems where *every* species in *every* problem occurs on exactly 2 patches.  Since this is ecologically unrealistic, we modified the XXXX method to allow the wrapping of nearly any species distribution around XXXX problems while preserving the known optimal solution.  The second ecologically unrealistic attribute of these problems is that there is no error or uncertainty in the occurrence and cost data.  Consequently, we built a series of simple models of false positive and false negative input errors and cost errors and applied them to the data.   

All of this gives us the skeleton for generating non-trivial reserve selection problems with known solutions and approximately known difficulty so that we can properly evaluate the performance of different reserve selectors.  However, what we would really like to know is, given a particular reserve selection problem and reserve selector, can we predict how much error is likely in the specific solution to this specific problem?  By error, we mean two things: 1) How much does it fall short of its representation targets? and 2) How much more does it cost than the correct optimal solution?  

To answer these questions for specific problems, we need to identify descriptors for each problem that are knowable or measurable without knowing the correct solution.  To our knowledge, the only descriptors people might currently use to guess the likely performance of a reserve selector would be related to the number of species and/or the number of patches.  We show here that, in our experiments, these are virtually useless as predictors of problem difficulty.  Instead, we reframe the problem as a bipartite network of species and patches, analogous to networks used in studying plant-pollinator interactions.  We then compute network metrics related to the relative complexity of these networks and show their utility in our experiments for predicting output errors in representation and cost.  


## Problem generation

###  XXXX base problem generation

We generated 

###  Wrapped problem generation

####  Lognormal

###  Input error generation

####  Input error types

####  Nominal vs. Realized errors

## Problem measures

###  Problem size

###  Graph/Network complexity measures

####  igraph package measures

####  bipartite package measures

Latapy redundancy measures and the web page used as the basis for the R code 

##  Problem difficulty measures

###  Output error measures

####  Fractional error

####  Error magnification

###  Run-time measures

##  Reserve selectors

###  Primary reserve selectors

###  Less important greedy selectors

##  Prediction 

###  Values to predict

###  Prediction methods

####  Linear models

####  Regression trees

####  Random forest

##  Running and managing many experiments

###  Number of experiments

###  Tzar

###  Nectar

#  Results

##  Input errors vs. output errors

###  Errors have huge variability regardless of input error amount and type

###  Optimization often amplifies input error

####  Error amplification

####  Error correction

##  Error predictability

###  Problem size is poor predictor

###  Graph measures appear to predict well

##  Discussion

##  Caveats

###  No generalization beyond this data set

Only intended as starting point to test the code and explore general behavior.

##  Lit review

###  Future work

####  Need broader random sample within Xu universe to generalize there

Running more experiments now.

####  Need more realistic distributions and problems (e.g., boundary lengths)

Will ask Marxan community for input files since there are lots of them applied to lots of real problems and all use the same common input file format and it's easy to anonymize.

#  Conclusion

##  Contributions
- Mapping of methods from theory of computation into reserve selection for problem generation accounting for
    - varying problem difficulty
    - known solutions
    - scalable generation time
- New method for wrapping Xu problems to make them more realistic for ecology
- Mapping bipartite network attributes into problem difficulty prediction in reserve selection
- Reasonably large first study with both easy and hard problems and tested predictions of error
- Reproducible data and R package code

##  Recommendations
- Test and predict the robustness of methods to uncertainty and problem difficulty
- Make optimization evaluation reflect true goals
    - If we're going to declare reserve selectors to be decision-*support* tools rather than decision-*making* tools, then evaluation functions used in optimization should reflect what it means for a solution or solution set to be optimal as decision support.  The current phrasing of optimality measures tacitly assume optimality is how well invidual solutions meet the cost and representation targets specified for them, independent of other solutions and of input uncertainty.  Instead, they should explicitly state and measure what it would mean for a single solution or a set of solutions to be optimal as decision advice.  For example, they could include things like:  
        - a measure of the *variety* in solutions produced (while still being decent individual solutions), or 
        - the measured *robustness* of solutions to relative problem difficulty and to uncertainty (of all kinds or specific kinds and amounts).

#  Software and data availability and reproducibility

##  R bdpg package on github

##  data on ???


#  References

#  Supplementary Material

