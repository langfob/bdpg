---
title: "Clean BDPG easy/hard data"
author: "Bill Langford"
date: '`r Sys.Date()`'
output: 
  html_notebook:
    toc: true # table of content true
    toc_depth: 4  # upto three depths of headings (specified by #, ## and ###)

---

###  Using this as a notebook instead of a knitr file for now

Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Cmd+Option+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Cmd+Shift+K* to preview the HTML file). 

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.

-----   

#  Load necessary libraries

-----   

```{r}
library (dplyr)

```

-----   

#  Set file paths

-----   

```{r setFilePaths}
#===============================================================================

# # # base_path = "/Users/bill/D/Projects/ProblemDifficulty/Results/bdpg_20_variants_all_rs_easy_base/bdpg_20_variants_all_rs_easy_base_Combined_err_amts/"
# 
#     #  Easy
easy_base_path = "/Users/bill/D/Projects/ProblemDifficulty/Results/bdpg_20_variants_all_rs_easy_base_2nd_attempt/bdpg_20_variants_all_rs_easy_base_2nd_try_Combined_err_amts/"
# 
#     #  Hard
# base_path = "/Users/bill/D/Projects/ProblemDifficulty/Results/bdpg_20_variants_all_rs_HARD_base_first_attempt/bdpg_20_variants_all_rs_HARD_base_1st_try_Combined_err_amts/"

    #  Hard - 2, 5, 7.5, 10 % error
hard_base_path = "/Users/bill/D/Projects/ProblemDifficulty/Results/bdpg_20_variants_all_rs_HARD_base_first_attempt/bdpg_20_variants_all_rs_HARD_base_02_05_075_10_Combined_err_amts/"

suffix = ".combined_results.csv"
```

-----   

#  Define functions

-----   

##  Define function to load input data

```{r loadInputData}
#===============================================================================

load_input_data <- function (rs_name, base_path, suffix)
    {
    infile = paste0 (base_path, rs_name, suffix)

        #  I can't remember why I called these "msa".  It might stand for 
        #  multi-set analysis...
    
    msa_dt         = read.csv (infile, header=TRUE, stringsAsFactors = FALSE)
    
    return (msa_dt)
    }

```

##  Define function to see which columns contain any NAs

Sometimes NAs indicate bad data, so find all columns that have NAs so that you can see whether they're OK.

### First try (and failure)

This code is taken from the blog post "Checking for NA with dplyr", posted on 
October 16, 2016 at https://sebastiansauer.github.io/NAs-with-dplyr/

The original code from that page is:

```
msa_dt %>% 
  select_if(function(x) any(is.na(x))) %>% 
  summarise_each(funs(sum(is.na(.)))) -> extra_NA
```

It gives an error:

```
`summarise_each()` is deprecated.
Use `summarise_all()`, `summarise_at()` or `summarise_if()` instead.
To map `funs` over all variables, use `summarise_all()`
package ‘bindrcpp’ was built under R version 3.4.4
```

###  Second try (and failure)

Also tried the following, which ran OK, but couldn't find a way to make it count the number of NAs along the way:

```
msa_dt %>% 
    summarise_if (function(x) any(is.na(x)), length) -> extra_NA
```

### Third try, doesn't use dplyr but it works (but not best)

So trying again using a completely different method based on some code in an answer at: https://stackoverflow.com/questions/24027605/determine-the-number-of-na-values-in-a-column.  

```
    #  Get the number of NAs in each column.

na_counts <- sapply (msa_dt, function(y) sum(is.na(y)))    #  Returns a vector w/ named elements
glimpse (na_counts)

    #  Get the number of NAs in only the columns that have at least one NA.

non_zero_counts = na_counts [which (na_counts != 0)]    #  Returns a vector w/ named elements
glimpse (non_zero_counts)

        #  Print actually makes it easier to see what the values are than glimpse,  
        #  but it takes up many rows of output, so I'm commenting it out for now.
#print (non_zero_counts)
```

###  Fourth try, solutions using dplyr (best yet)

Different ways to count NAs over multiple columns  
September 08, 2017  

https://sebastiansauer.github.io/sum-isna/

Here, derive na_counts using "Way 3: using dplyr" from that web page.  This gives the most  readable display of the output I've found yet.  Will go with this one for now.

Unfortunately, it returns 64 columns containing NAs !!  

It actually looks like it might be more informative to run this on each variatn of the data separately because many values match the row count for one or the other.  That can flag values that are purposely not set for that variant or ones that I forgot to set.  Other columns that have counts that don't match the number of rows may indicate similar kinds of things.

```{r defineFuncToFindNAcols}
getColsWithNonZeroNAct <- function (aDataframe)
    {
    aDataframe %>%
      select (everything()) %>%  # replace to your needs
      summarise_all (funs (sum (is.na (.)))) -> na_counts
    
    non_zero_counts = na_counts [which (na_counts != 0)]    #  Returns a vector w/ named elements
    glimpse (non_zero_counts)
    
    return (na_counts)
    }
```


-----   

#  Load input data    

-----   

Note that each reserve selector needs to be treated separately because each one may have columns that are specific to that reserve selector.  For example, gurobi has mipgap information that no other reserve selector has.

```{r loadAndSummarizeInputData}

rs_name = "Gurobi"
#rs_name = "Marxan_SA"
#rs_name = "Marxan_SA_SS"
#rs_name = "ZL_Backward"
#rs_name = "ZL_Forward"
#rs_name = "SR_Backward"
#rs_name = "SR_Forward"
#rs_name = "UR_Backward"
#rs_name = "UR_Forward"

easy_msa_dt = load_input_data (rs_name, easy_base_path, suffix)
#glimpse (easy_msa_dt)


hard_msa_dt = load_input_data (rs_name, hard_base_path, suffix)
#glimpse (hard_msa_dt)
```

-----   

# Combine the easy and hard data into one data frame

```{r combineEasyHard}
msa_dt = bind_rows (easy_msa_dt, hard_msa_dt,  .id = "id")
#glimpse (msa_dt)
    #  Convert dplyr's automatically generated dataset ids of "1" and "2" to 
    #  "Easy" and "Hard".
msa_dt$id = sapply (msa_dt$id, function (x) if (x == "1") "Easy" else "Hard")
```

#  See which columns contain any NAs

Sometimes NAs indicate bad data, so find all columns that have NAs so that you can see whether they're OK.

```{r findColsWithNAsUsingdplyr}
getColsWithNonZeroNAct (easy_msa_dt)
getColsWithNonZeroNAct (hard_msa_dt)
getColsWithNonZeroNAct (msa_dt)
```



-----  

#  Add metadata columns to mark row removals & modifications  

-----  

While cleaning the data, some rows may be removed or modified.  To document what's been and where to find where it was done, do two things.  First, add a column called "cln_mod" to the original data set.  Second, create a new, empty data set that will all lines removed from the data set during cleaning.  

Initialize all "cln_mod" values in the original data set to "keep".  

If the line is removed at some point, replace "keep" with a string naming the step where that line is removed and copy the line to the removed data set.  It doesn't matter what those strings are; they're just anything that enables you to find that processing step in this document.  

Same idea for modifications to a line, but don't remove it and instead, set the cln_mod value to a label string.  

Originally, I had a column for removals and a column for modifications, but it seemed difficult to manage for removals because they have to be tracked separately from the evolving data since they disappear from the evolving data.  This would make it cumbersome to find the matching lines in the original and evolving data sets.  Modified lines could be tracked in the evolving data set without any problem.  

```{r addClnCols}
save_removed_lines <- 
        function (all_removed_lines,  #  dataframe:  existing cache of previously removed lines
                  new_removed_lines,  #  dataframe:  lines currently being removed from original data
                  label)              #  string:     label indicating where in cleaning process the lines are removed from the original data
    {
        #  If no lines to remove are passed in, then do nothing to the 
        #  existing cache of removed lines.
    
    if (! is.null (new_removed_lines))
        {
            #  Attach a label to the removed lines to indicate where they 
            #  were removed during the data cleaning process.
        
        new_removed_lines$cln_mod = label
        
            #  If there's already a data frame of previously removed lines, 
            #  then add to it.
            #  Otherwise, initialize it as the newly removed lines.
        
        all_removed_lines = 
            if (is.null (all_removed_lines))  new_removed_lines  else  
                                              bind_rows (all_removed_lines)
        }
    
    return (all_removed_lines)
    }

msa_dt %>% 
    mutate (cln_mod = "keep") -> msa_dt
glimpse (msa_dt)
```

#  Count number of problems in each problem class

This page mentions that you can group by more than one variable at a time, which I didn't realize:  

https://data-se.netlify.com/2017/06/28/second_look_group_by/

```{r groupTheData}
msa_dt %>% 
    group_by (id, rsp_cor_or_app_str, rsp_base_wrap_str, rsp_combined_err_label) %>% 
    summarise (n_per_group = n()) -> group_cts
#glimpse (group_cts)
print (group_cts)

```

#  Compute error magnifications and count resulting NaNs

```{r computeErrMags}
msa_dt %>% 
    mutate (emag = rsr_COR_euc_out_err_frac / 
                   rsp_euc_realized_Ftot_and_cost_in_err_frac) -> msa_dt

                                # rsp_num_PUs, 
                                # rsp_num_spp, 

                                # rsp_FP_const_rate, 
                                # rsp_FN_const_rate, 
                                # rsp_realized_FP_rate, 
                                # rsp_realized_FN_rate, 
                                # rsp_realized_Ftot_rate, 

msa_dt %>%
    filter (is.nan (emag)) %>% 
    select (id, rsp_cor_or_app_str, rsp_base_wrap_str, rsp_combined_err_label, rsp_num_spp, rsp_num_PUs, rsp_FN_const_rate, rsr_COR_euc_out_err_frac, rsp_euc_realized_Ftot_and_cost_in_err_frac) -> emag_nan_lines

emag_nan_lines %>%            
    group_by (id, rsp_cor_or_app_str, rsp_base_wrap_str, rsp_combined_err_label) %>% 
    summarise (n_per_group = n()) -> emag_nan_lines_group_cts
print (emag_nan_lines_group_cts)
```

### There should only be NaNs for COR problems, but there some for Easy App problems.

Are they just incredibly small input errors in the denominator (since they're all dominated by FN values)?

Looking more closely, it looks like these have 0/0 for the mag.  These may be a result of how I'm drawing instances of FNs and FPs.  I think that I'm drawing a uniform random number and seeing if it's less than the nominal error rate, e.g., 2%.  If you do this enough times, you may end up randomly getting the occasional case (particularly in small problems) where you never get a random number under the nominal value, so you get 0 realized input error.  Also, 0 input error leads to 0 output error in most cases, so you'd get the 0/0 magnification.  

(Note that the nominal FN rate is 2% for all except the last 4 cases, which are all Base problems with 3 at 5% and 1 at 7.5%, all having between 34 and 37 species, which have 2 Positives for each species, meaning between 68 and 74 Positives that would all have to draw runif() values greater than the nominal value.  That seems unlikely, but maybe it's possible for the idea in the previous paragraph to hold.)

An alternative method would be to use safe_sample() instead of runif() and draw indices of exactly 2% of the Positives to turn into False Negatives.  That way, you'd never get 0 input error (assuming there were enough Positives so that 2% of them was >= 1, i.e., at least 50 Positives for the example of 2%).  I like the current method though because it gets you values scattered around the nominal value rather than all being exactly the nominal value.  This gives a bit more variation in training examples to learn from.  However, you could get the same result by just scattering the nominal values themselves instead.  

Need to look more closely at the code to see if this is what's going on...

```{r examineAppMagNaNs}

    #  Find the APP lines that have NaN emag values.
emag_nan_lines %>%
    filter (rsp_cor_or_app_str == "APP") -> app_emag_nan_lines
glimpse (app_emag_nan_lines)
print (app_emag_nan_lines)
```

For the moment, I'm going to just remove those Easy App problems with NaNs.

```{r removeAppMagNaNs}
    #  Copy and save the lines to be deleted.
msa_dt %>%
    filter (id == "Easy" & rsp_cor_or_app_str == "APP" & is.nan (emag)) -> easy_app_nan_mag_lines
cat ("\n\n\n\neasy_app_nan_mag_lines\n\n")
glimpse (easy_app_nan_mag_lines)

removed_msa_dt_lines = NULL
removed_msa_dt_lines = save_removed_lines (removed_msa_dt_lines, 
                                           easy_app_nan_mag_lines, 
                                           "removeAppMagNaNs")
cat ("\n\n\n\nremoved_msa_dt_lines\n\n")
glimpse (removed_msa_dt_lines)

    #  Now, remove those lines from the original data set.
msa_dt %>%
    filter (! (id == "Easy" & rsp_cor_or_app_str == "APP" & is.nan (emag))) -> msa_dt
cat ("\n\n\n\nmsa_dt after easy_app_nan_mag_lines removed\n\n")
glimpse (msa_dt)
```

-----  
#  Not ready to do these yet...
---  

#  Separate COR and APP data
```{r separateCorAndAppdata}

                # rs_method_name, 
                # rsp_combined_err_label, 
                # rsp_cor_or_app_str, 
                # rsp_base_wrap_str, 

# COR_easy_msa_dt = filter (easy_msa_dt, rsp_cor_or_app_str == "COR")
# #glimpse (COR_easy_msa_dt)
# APP_easy_msa_dt = filter (easy_msa_dt, rsp_cor_or_app_str == "APP")
# #glimpse (APP_easy_msa_dt)

# COR_hard_msa_dt = filter (hard_msa_dt, rsp_cor_or_app_str == "COR")
# #glimpse (COR_hard_msa_dt)
# APP_hard_msa_dt = filter (hard_msa_dt, rsp_cor_or_app_str == "APP")
# #glimpse (APP_hard_msa_dt)

# getColsWithNonZeroNAct (COR_easy_msa_dt)
# getColsWithNonZeroNAct (APP_easy_msa_dt)
# getColsWithNonZeroNAct (COR_hard_msa_dt)
# getColsWithNonZeroNAct (APP_hard_msa_dt)
```




