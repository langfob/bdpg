% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/compute_solution_scores.R
\name{compute_solution_vector_scores}
\alias{compute_solution_vector_scores}
\title{Compute vector of classification-style scores for a candidate solution with
respect to a reference species occupancy matrix (i.e., correct or apparent)}
\usage{
compute_solution_vector_scores(ref_spp_occ_matrix, num_PUs, cand_sol_PU_IDs,
  num_PUs_in_cand_solution, num_PUs_in_optimal_solution, spp_rep_targets,
  num_spp, input_err_FP = 0, input_err_FN = 0)
}
\arguments{
\item{ref_spp_occ_matrix}{reference species occupancy matrix, e.g.,
correct or apparent species occupancy matrix}

\item{num_PUs}{integer number of planning units}

\item{cand_sol_PU_IDs}{vector of only the planning unit IDs that are
included in the candidate solution}

\item{num_PUs_in_cand_solution}{integer number of planning units in the
candidate solution}

\item{num_PUs_in_optimal_solution}{integer number of planning units in the
optimal solution}

\item{spp_rep_targets}{vector of numeric species representation targets}

\item{num_spp}{integer number of species in the problem}

\item{input_err_FP}{fractional amount of False Positive error added to the
correct problem to create the apparent problem}

\item{input_err_FN}{fractional amount of False Negative error added to the
correct problem to create the apparent problem}
}
\value{
list of solution vector scores
}
\description{
Computes error measures related to confusion matrix, etc. For the purpose of
 computing a performance score, start by treating the problem as if it's a
 classification problem, where a selected patch is classified as 1 and an
 unselected patch is classified as 0. This allows us to use any of the many
 existing measures developed for classifiers.
}
\details{
Doesn't matter too much which measures we use, since this is mostly about
 demonstrating how to generate and evaluate problems and users will have to
 choose which measure best aligns with their own goals.

However, it may be that some of these measures are easier to learn to
 predict than others, so it's good to provide several different ones until we
 know more. The base case would be to provide the ones that are the simplest
 and most direct measures over the confusion matrix.
 Confusion matrix fractions

Note that the TP and TN values are computed as the min of the candidate and
 correct values. This is because the number of "trues" for the candidate
 can't exceed the number of "trues" in the correct solution by definition.

Similarly, any count of TP or TN that falls short of the corresponding
 counts in the correct solution represents the number of TP or TN that the
 candidate got right and using the number of TP or TN from the correct would
 overstate the candidate's performance.

This all seems a bit odd in the normal classification context because in
 classification, you would have to know _which_ PUs the classifier got right
 and count them up. In reserve selection, there could be many ways to get the
 same final optimal count of PUs in the solution and we don't care _which_
 ones are chosen to get that count. However, the same kind of a scoring
 system can work because we know that anything short of the optimal number
 represents the existance of False Negatives, i.e., _some_ PUs who should
 have been included.  Similarly, any count greater than the optimal count
 implies the existance of False Positives, i.e., _some_ PUs who should NOT
 have been included.

Since nearly all classification performance scores are based on some
 combination of the 4 values from the confusion matrix (TP,TN,FP,FN),
 choosing those 4 values sets us up to compute all these scores.
}

